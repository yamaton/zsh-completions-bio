#compdef _salmon salmon

# Auto-generated with h2o

    function _salmon_index {
        _arguments \
            {-v,--version}'[print version string]' \
            {-h,--help}'[produce help message]' \
            {-t,--transcripts}'[Transcript fasta file.]' \
            {-k,--kmerLen}'[The size of k-mers that should be used for the quasi index. (Default: 31)]' \
            {-i,--index}'[salmon index.]' \
            '--gencode[This flag will expect the input transcript fasta to be in GENCODE format, and will split the transcript name at the first '\''|'\'' character. These reduced names will be used in the output and when looking for these transcripts in a gene to transcript GTF.]' \
            '--features[This flag will expect the input reference to be in the tsv file format, and will split the feature name at the first '\''tab'\'' character. These reduced names will be used in the output and when looking for the sequence of the features.GTF.]' \
            '--keepDuplicates[This flag will disable the default indexing behavior of discarding sequence-identical duplicate transcripts. If this flag is passed, then duplicate transcripts that appear in the input will be retained and quantified separately.]' \
            {-p,--threads}'[Number of threads to use during indexing. (Default: 2)]' \
            '--keepFixedFasta[Retain the fixed fasta file (without short transcripts and duplicates, clipped, etc.) generated during indexing]' \
            {-f,--filterSize}'[The size of the Bloom filter that will be used by TwoPaCo during indexing. The filter will be of size 2^{filterSize}. The default value of -1 means that the filter size will be automatically set based on the number of distinct k-mers in the input, as estimated by nthll. (Default: -1)]' \
            '--tmpdir[The directory location that will be used for TwoPaCo temporary files; it will be created if need be and be removed prior to indexing completion. The default value will cause a (temporary) subdirectory of the salmon index directory to be used for this purpose.]' \
            '--sparse[Build the index using a sparse sampling of k-mer positions This will require less memory (especially during quantification), but will take longer to construct and can slow down mapping / alignment]' \
            {-d,--decoys}'[Treat these sequences ids from the reference as the decoys that may have sequence homologous to some known transcript. for example in case of the genome, provide a list of chromosome name --- one per line]' \
            {-n,--no-clip}'[Don'\''t clip poly-A tails from the ends of target sequences]' \
            '--type[The type of index to build; the only option is "puff" in this version of salmon.]' \
            "*: :_files"

    }

    function _salmon_quant {
        _arguments \
            {-l,--libType}'[Format string describing the library type]' \
            {-i,--index}'[salmon index]' \
            {-r,--unmatedReads}'[List of files containing unmated reads of (e.g. single-end reads)]' \
            {-1,--mates1}'[File containing the #1 mates]' \
            {-2,--mates2}'[File containing the #2 mates]' \
            {-v,--version}'[print version string]' \
            {-h,--help}'[produce help message]' \
            {-o,--output}'[Output quantification directory.]' \
            '--seqBias[Perform sequence-specific bias correction.]' \
            '--gcBias[\[beta for single-end reads\] Perform fragment GC bias correction.]' \
            '--posBias[Perform positional bias correction.]' \
            {-p,--threads}'[The number of threads to use concurrently. (Default: 8)]' \
            '--incompatPrior[This option sets the prior probability that an alignment that disagrees with the specified library type (--libType) results from the true fragment origin. Setting this to 0 specifies that alignments that disagree with the library type should be "impossible", while setting it to 1 says that alignments that disagree with the library type are no less likely than those that do. (Default: 0)]' \
            {-g,--geneMap}'[File containing a mapping of transcripts to genes. If this file is provided salmon will output both quant.sf and quant.genes.sf files, where the latter contains aggregated gene-level abundance estimates. The transcript to gene mapping should be provided as either a GTF file, or a in a simple tab-delimited format where each line contains the name of a transcript and the gene to which it belongs separated by a tab. The extension of the file is used to determine how the file should be parsed. Files ending in '\''.gtf'\'', '\''.gff'\'' or '\''.gff3'\'' are assumed to be in GTF format; files with any other extension are assumed to be in the simple format. In GTF / GFF format, the "transcript_id" is assumed to contain the transcript identifier and the "gene_id" is assumed to contain the corresponding gene identifier.]' \
            '--auxTargetFile[A file containing a list of "auxiliary" targets. These are valid targets (i.e., not decoys) to which fragments are allowed to map and be assigned, and which will be quantified, but for which auxiliary models like sequence-specific and fragment-GC bias correction should not be applied.]' \
            '--meta[If you'\''re using Salmon on a metagenomic dataset, consider setting this flag to disable parts of the abundance estimation model that make less sense for metagenomic data.]' \
            '--discardOrphansQuasi[\[selective-alignment mode only\] : Discard orphan mappings in selective-alignment mode. If this flag is passed then only paired mappings will be considered toward quantification estimates. The default behavior is to consider orphan mappings if no valid paired mappings exist. This flag is independent of the option to write the orphaned mappings to file (--writeOrphanLinks).]' \
            '--discardOrphans[\[alignment-based mode only\] : Discard orphan alignments in the input . If this flag is passed, then only paired alignments will be considered toward quantification estimates. The default behavior is to consider orphan alignments if no valid paired mappings exist.]' \
            '--validateMappings[\[*deprecated* (no effect; selective-alignment is the default)\]]' \
            '--consensusSlack[\[selective-alignment mode only\] : The amount of slack allowed in the selective-alignment filtering mechanism. If this is set to a fraction, X, greater than 0 (and in \[0,1)), then uniMEM chains with scores below (100 * X)% of the best chain score for a read, and read pairs with a sum of chain scores below (100 * X)% of the best chain score for a read pair will be discounted as a mapping candidates. The default value of this option is 0.35.]' \
            '--preMergeChainSubThresh[\[selective-alignment mode only\] : The threshold of sub-optimal chains, compared to the best chain on a given target, that will be retained and passed to the next phase of mapping. Specifically, if the best chain for a read (or read-end in paired-end mode) to target t has score X_t, then all chains for this read with score >= X_t * preMergeChainSubThresh will be retained and passed to subsequent mapping phases. This value must be in the range \[0, 1\]. (Default: 0.75)]' \
            '--postMergeChainSubThresh[\[selective-alignment mode only\] : The threshold of sub-optimal chain pairs, compared to the best chain pair on a given target, that will be retained and passed to the next phase of mapping. This is different than preMergeChainSubThresh, because this is applied to pairs of chains (from the ends of paired-end reads) after merging (i.e. after checking concordancy constraints etc.). Specifically, if the best chain pair to target t has score X_t, then all chain pairs for this read pair with score >= X_t * postMergeChainSubThresh will be retained and passed to subsequent mapping phases. This value must be in the range \[0, 1\]. Note: This option is only meaningful for paired-end libraries, and is ignored for single-end libraries. (Default: 0.9)]' \
            '--orphanChainSubThresh[\[selective-alignment mode only\] : This threshold sets a global sub-optimality threshold for chains corresponding to orphan mappings. That is, if the merging procedure results in no concordant mappings then only orphan mappings with a chain score >= orphanChainSubThresh * bestChainScore will be retained and passed to subsequent mapping phases. This value must be in the range \[0, 1\]. Note: This option is only meaningful for paired-end libraries, and is ignored for single-end libraries. (Default: 0.95)]' \
            '--scoreExp[\[selective-alignment mode only\] : The factor by which sub-optimal alignment scores are downweighted to produce a probability. If the best alignment score for the current read is S, and the score for a particular alignment is w, then the probability will be computed porportional to exp( -scoreExp * (S-w) ). (Default: 1)]' \
            '--minScoreFraction[\[selective-alignment mode only\] : The fraction of the optimal possible alignment score that a mapping must achieve in order to be considered "valid" --- should be in (0,1\]. Salmon Default 0.65 and Alevin Default 0.87]' \
            '--mismatchSeedSkip[\[selective-alignment mode only\] : After a k-mer hit is extended to a uni-MEM, the uni-MEM extension can terminate for one of 3 reasons; the end of the read, the end of the unitig, or a mismatch. If the extension ends because of a mismatch, this is likely the result of a sequencing error. To avoid looking up many k-mers that will likely fail to be located in the index, the search procedure skips by a factor of mismatchSeedSkip until it either (1) finds another match or (2) is k-bases past the mismatch position. This value controls that skip length. A smaller value can increase sensitivity, while a larger value can speed up seeding. (Default: 3)]' \
            '--disableChainingHeuristic[\[selective-alignment mode only\] : By default, the heuristic of (Li 2018) is implemented, which terminates the chaining DP once a given number of valid backpointers are found. This speeds up the seed (MEM) chaining step, but may result in sub-optimal chains in complex situations (e.g. sequences with many repeats and overlapping repeats). Passing this flag will disable the chaining heuristic, and perform the full chaining dynamic program, guaranteeing the optimal chain is found in this step.]' \
            '--decoyThreshold[\[selective-alignment mode only\] : For an alignemnt to an annotated transcript to be considered invalid, it must have an alignment score < (decoyThreshold * bestDecoyScore). A value of 1.0 means that any alignment strictly worse than the best decoy alignment will be discarded. A smaller value will allow reads to be allocated to transcripts even if they strictly align better to the decoy sequence. (Default: 1)]' \
            '--ma[\[selective-alignment mode only\] : The value given to a match between read and reference nucleotides in an alignment. (Default: 2)]' \
            '--mp[\[selective-alignment mode only\] : The value given to a mis-match between read and reference nucleotides in an alignment. (Default: -4)]' \
            '--go[\[selective-alignment mode only\] : The value given to a gap opening in an alignment. (Default: 6)]' \
            '--ge[\[selective-alignment mode only\] : The value given to a gap extension in an alignment. (Default: 2)]' \
            '--bandwidth[\[selective-alignment mode only\] : The value used for the bandwidth passed to ksw2. A smaller bandwidth can make the alignment verification run more quickly, but could possibly miss valid alignments. (Default: 15)]' \
            '--allowDovetail[\[selective-alignment mode only\] : allow dovetailing mappings.]' \
            '--recoverOrphans[\[selective-alignment mode only\] : Attempt to recover the mates of orphaned reads. This uses edlib for orphan recovery, and so introduces some computational overhead, but it can improve sensitivity.]' \
            '--mimicBT2[\[selective-alignment mode only\] : Set flags to mimic parameters similar to Bowtie2 with --no-discordant and --no-mixed flags. This increases disallows dovetailing reads, and discards orphans. Note, this does not impose the very strict parameters assumed by RSEM+Bowtie2, like gapless alignments. For that behavior, use the --mimiStrictBT2 flag below.]' \
            '--mimicStrictBT2[\[selective-alignment mode only\] : Set flags to mimic the very strict parameters used by RSEM+Bowtie2. This increases --minScoreFraction to 0.8, disallows dovetailing reads, discards orphans, and disallows gaps in alignments.]' \
            '--softclip[\[selective-alignment mode only (experimental)\] : Allos soft-clipping of reads during selective-alignment. If this option is provided, then regions at the beginning or end of the read can be withheld from alignment without any effect on the resulting score (i.e. neither adding nor removing from the score). This will drastically reduce the penalty if there are mismatches at the beginning or end of the read due to e.g. low-quality bases or adapters. NOTE: Even with soft-clipping enabled, the read must still achieve a score of at least minScoreFraction * maximum achievable score, where the maximum achievable score is computed based on the full (un-clipped) read length.]' \
            '--softclipOverhangs[\[selective-alignment mode only\] : Allow soft-clipping of reads that overhang the beginning or ends of the transcript. In this case, the overhaning section of the read will simply be unaligned, and will not contribute or detract from the alignment score. The default policy is to force an end-to-end alignment of the entire read, so that overhanings will result in some deletion of nucleotides from the read.]' \
            '--fullLengthAlignment[\[selective-alignment mode only\] : Perform selective alignment over the full length of the read, beginning from the (approximate) initial mapping location and using extension alignment.]' \
            '--hardFilter[\[selective-alignemnt mode only\] : Instead of weighting mappings by their alignment score, this flag will discard any mappings with sub-optimal alignment score. The default option of soft-filtering (i.e. weighting mappings by their alignment score) usually yields slightly more accurate abundance estimates but this flag may be desirable if you want more accurate '\''naive'\'' equivalence classes, rather than range factorized equivalence classes.]' \
            '--minAlnProb[\[selective-alignment mode only\] : Any mapping whose alignment probability (as computed by P(aln) = exp(-scoreExp * difference from best mapping score) is less than minAlnProb will not be considered as a valid alignment for this read. The goal of this flag is to remove very low probability alignments that are unlikely to have any non-trivial effect on the final quantifications. Filtering such alignments reduces the number of variables that need to be considered and can result in slightly faster inference and '\''cleaner'\'' equivalence classes. (Default: 1.0-5)]' \
            {-z,--writeMappings}'[If this option is provided, then the selective-alignment results will be written out in SAM-compatible format. By default, output will be directed to stdout, but an alternative file name can be provided instead. (Default: -)]' \
            '--hitFilterPolicy[\[selective-alignment mode only\] : Determines the policy by which hits are filtered in selective alignment. Filtering hits after chaining (the default) is more sensitive, but more computationally intensive, because it performs the chaining dynamic program for all hits. Filtering before chaining is faster, but some true hits may be missed. The options are BEFORE, AFTER, BOTH and NONE. (Default: AFTER)]' \
            '--noErrorModel[Turn off the alignment error model, which takes into account the the observed frequency of different types of mismatches / indels when computing the likelihood of a given alignment. Turning this off can speed up alignment-based salmon, but can harm quantification accuracy.]' \
            '--numErrorBins[The number of bins into which to divide each read when learning and applying the error model. For example, a value of 10 would mean that effectively, a separate error model is leared and applied to each 10th of the read, while a value of 3 would mean that a separate error model is applied to the read beginning (first third), middle (second third) and end (final third). (Default: 6)]' \
            {-s,--sampleOut}'[Write a "postSample.bam" file in the output directory that will sample the input alignments according to the estimated transcript abundances. If you'\''re going to perform downstream analysis of the alignments with tools which don'\''t, themselves, take fragment assignment ambiguity into account, you should use this output.]' \
            {-u,--sampleUnaligned}'[In addition to sampling the aligned reads, also write the un-aligned reads to "postSample.bam".]' \
            '--gencode[This flag will expect the input transcript fasta to be in GENCODE format, and will split the transcript name at the first '\''|'\'' character. These reduced names will be used in the output and when looking for these transcripts in a gene to transcript GTF.]' \
            '--scoreExp[The factor by which sub-optimal alignment scores are downweighted to produce a probability. If the best alignment score for the current read is S, and the score for a particular alignment is w, then the probability will be computed porportional to exp( - scoreExp * (S-w) ). NOTE: This flag only has an effect if you are parsing alignments produced by salmon itself (i.e. pufferfish or RapMap in selective-alignment mode). (Default: 1)]' \
            '--mappingCacheMemoryLimit[If the file contained fewer than this many mapped reads, then just keep the data in memory for subsequent rounds of inference. Obviously, this value should not be too large if you wish to keep a low memory usage, but setting it large enough to accommodate all of the mapped read can substantially speed up inference on "small" files that contain only a few million reads. (Default: 2000000)]' \
            '--alternativeInitMode[\[Experimental\]: Use an alternative strategy (rather than simple interpolation between) the online and uniform abundance estimates to initialize the EM / VBEM algorithm.]' \
            '--auxDir[The sub-directory of the quantification directory where auxiliary information e.g. bootstraps, bias parameters, etc. will be written. (Default: aux_info)]' \
            '--skipQuant[Skip performing the actual transcript quantification (including any Gibbs sampling or bootstrapping).]' \
            '--dumpEq[Dump the simple equivalence class counts that were computed during mapping or alignment.]' \
            {-d,--dumpEqWeights}'[Dump conditional probabilities associated with transcripts when equivalence class information is being dumped to file. Note, this will dump the factorization that is actually used by salmon'\''s offline phase for inference. If you are using range-factorized equivalence classes (the default) then the same transcript set may appear multiple times with different associated conditional probabilities.]' \
            '--minAssignedFrags[The minimum number of fragments that must be assigned to the transcriptome for quantification to proceed.(Default: 10)]' \
            '--reduceGCMemory[If this option is selected, a more memory efficient (but slightly slower) representation is used to compute fragment GC content. Enabling this will reduce memory usage, but can also reduce speed. However, the results themselves will remain the same.]' \
            '--biasSpeedSamp[The value at which the fragment length PMF is down-sampled when evaluating sequence-specific & GC fragment bias. Larger values speed up effective length correction, but may decrease the fidelity of bias modeling results.(Default: 5)]' \
            '--fldMax[The maximum fragment length to consider when building the empirical distribution. (Default: 1000)]' \
            '--fldMean[The mean used in the fragment length distribution prior. (Default: 250)]' \
            '--fldSD[The standard deviation used in the fragment length distribution prior. (Default: 25)]' \
            {-f,--forgettingFactor}'[The forgetting factor used in the online learning schedule. A smaller value results in quicker learning, but higher variance and may be unstable. A larger value results in slower learning but may be more stable. Value should be in the interval (0.5, 1.0\]. (Default: 0.65)]' \
            '--initUniform[initialize the offline inference with uniform parameters, rather than seeding with online parameters.]' \
            '--maxOccsPerHit[When collecting "hits" (MEMs), hits having more than maxOccsPerHit occurrences won'\''t be considered. (Default: 1000)]' \
            {-w,--maxReadOcc}'[Reads "mapping" to more than this many places won'\''t be considered. (Default: 200)]' \
            '--maxRecoverReadOcc[Relevant for alevin with '\''--sketch'\'' mode only: if a read has valid seed matches, but no read has matches leading to fewer than "maxReadOcc" mappings, then try to recover mappings for this read as long as there are fewer than "maxRecoverReadOcc" mappings. (Default: 2500)]' \
            '--noLengthCorrection[\[experimental\] : Entirely disables length correction when estimating the abundance of transcripts. This option can be used with protocols where one expects that fragments derive from their underlying targets without regard to that target'\''s length (e.g. QuantSeq)]' \
            '--noEffectiveLengthCorrection[Disables effective length correction when computing the probability that a fragment was generated from a transcript. If this flag is passed in, the fragment length distribution is not taken into account when computing this probability.]' \
            '--noSingleFragProb[Disables the estimation of an associated fragment length probability for single-end reads or for orphaned mappings in paired-end libraries. The default behavior is to consider the probability of all possible fragment lengths associated with the retained mapping. Enabling this flag (i.e. turning this default behavior off) will simply not attempt to estimate a fragment length probability in such cases.]' \
            '--noFragLengthDist[\[experimental\] : Don'\''t consider concordance with the learned fragment length distribution when trying to determine the probability that a fragment has originated from a specified location. Normally, Fragments with unlikely lengths will be assigned a smaller relative probability than those with more likely lengths. When this flag is passed in, the observed fragment length has no effect on that fragment'\''s a priori probability.]' \
            '--noBiasLengthThreshold[\[experimental\] : If this option is enabled, then no (lower) threshold will be set on how short bias correction can make effective lengths. This can increase the precision of bias correction, but harm robustness. The default correction applies a threshold.]' \
            '--numBiasSamples[Number of fragment mappings to use when learning the sequence-specific bias model. (Default: 2000000)]' \
            '--numAuxModelSamples[The first <numAuxModelSamples> are used to train the auxiliary model parameters (e.g. fragment length distribution, bias, etc.). After ther first <numAuxModelSamples> observations the auxiliary model parameters will be assumed to have converged and will be fixed. (Default: 5000000)]' \
            '--numPreAuxModelSamples[The first <numPreAuxModelSamples> will have their assignment likelihoods and contributions to the transcript abundances computed without applying any auxiliary models. The purpose of ignoring the auxiliary models for the first <numPreAuxModelSamples> observations is to avoid applying these models before their parameters have been learned sufficiently well. (Default: 5000)]' \
            '--useEM[Use the traditional EM algorithm for optimization in the batch passes.]' \
            '--useVBOpt[Use the Variational Bayesian EM \[default\]]' \
            '--rangeFactorizationBins[Factorizes the likelihood used in quantification by adopting a new notion of equivalence classes based on the conditional probabilities with which fragments are generated from different transcripts. This is a more fine-grained factorization than the normal rich equivalence classes. The default value (4) corresponds to the default used in Zakeri et al. 2017 (doi: 10.1093/bioinformatics/btx262), and larger values imply a more fine-grained factorization. If range factorization is enabled, a common value to select for this parameter is 4. A value of 0 signifies the use of basic rich equivalence classes.]' \
            '--numGibbsSamples[Number of Gibbs sampling rounds to perform. (Default: 0)]' \
            '--noGammaDraw[This switch will disable drawing transcript fractions from a Gamma distribution during Gibbs sampling. In this case the sampler does not account for shot-noise, but only assignment ambiguity]' \
            '--numBootstraps[Number of bootstrap samples to generate. Note: This is mutually exclusive with Gibbs sampling. (Default: 0)]' \
            '--bootstrapReproject[This switch will learn the parameter distribution from the bootstrapped counts for each sample, but will reproject those parameters onto the original equivalence class counts.]' \
            '--thinningFactor[Number of steps to discard for every sample kept from the Gibbs chain. The larger this number, the less chance that subsequent samples are auto-correlated, but the slower sampling becomes. (Default: 16)]' \
            {-q,--quiet}'[Be quiet while doing quantification (don'\''t write informative output to the console unless something goes wrong).]' \
            '--perTranscriptPrior[The prior (either the default or the argument provided via --vbPrior) will be interpreted as a transcript-level prior (i.e. each transcript will be given a prior read count of this value)]' \
            '--perNucleotidePrior[The prior (either the default or the argument provided via --vbPrior) will be interpreted as a nucleotide-level prior (i.e. each nucleotide will be given a prior read count of this value)]' \
            '--sigDigits[The number of significant digits to write when outputting the EffectiveLength and NumReads columns. (Default: 3)]' \
            '--vbPrior[The prior that will be used in the VBEM algorithm. This is interpreted as a per-transcript prior, unless the --perNucleotidePrior flag is also given. If the --perNucleotidePrior flag is given, this is used as a nucleotide-level prior. If the default is used, it will be divided by 1000 before being used as a nucleotide-level prior, i.e. the default per-nucleotide prior will be 1e-5. (Default: 0.01)]' \
            '--writeOrphanLinks[Write the transcripts that are linked by orphaned reads.]' \
            '--writeUnmappedNames[Write the names of un-mapped reads to the file unmapped_names.txt in the auxiliary directory.]' \
            "*: :_files"

    }

    function _salmon_alevin {
        _arguments \
            {-l,--libType}'[Format string describing the library type]' \
            {-i,--index}'[salmon index]' \
            {-r,--unmatedReads}'[List of files containing unmated reads of (e.g. single-end reads)]' \
            {-1,--mates1}'[File containing the #1 mates]' \
            {-2,--mates2}'[File containing the #2 mates]' \
            {-v,--version}'[print version string]' \
            {-h,--help}'[produce help message]' \
            {-o,--output}'[Output quantification directory.]' \
            {-j,--rad}'[just selectively align the data and write the results to a RAD file.  Do not perform the rest of the quantification procedure.]' \
            '--sketch[perform sketching rather than selective alignment and write the results to a RAD file. Requires the `--rad` flag. Do not perform the rest of the quantification procedure.]' \
            {-p,--threads}'[The number of threads to use concurrently.]' \
            '--tgMap[transcript to gene map tsv file]' \
            '--hash[Secondary input point for Alevin using Big freaking Hash (bfh.txt) file. Works Only with --chromium]' \
            '--dropseq[Use DropSeq Single Cell protocol for the library]' \
            '--chromiumV3[Use 10x chromium v3 Single Cell protocol for the library.]' \
            '--chromium[Use 10x chromium v2 Single Cell protocol for the library.]' \
            '--gemcode[Use 10x gemcode v1 Single Cell protocol for the library.]' \
            '--citeseq[Use CITESeq Single Cell protocol for the library, 16 CB, 12 UMI and features.]' \
            '--celseq[Use CEL-Seq Single Cell protocol for the library.]' \
            '--celseq2[Use CEL-Seq2 Single Cell protocol for the library.]' \
            '--splitseqV1[Use Split-SeqV1 Single Cell protocol for the library.]' \
            '--splitseqV2[Use Split-SeqV2 Single Cell protocol for the library.]' \
            '--quartzseq2[Use Quartz-Seq2 v3.2 Single Cell protocol for the library assumes 15 length barcode and 8 length UMI.]' \
            '--sciseq3[Use sci-RNA-seq3 protocol for the library.]' \
            '--whitelist[File containing white-list barcodes]' \
            '--featureStart[This flag should be used with citeseq and specifies the starting index of the feature barcode on Read2.]' \
            '--featureLength[This flag should be used with citeseq and specifies the length of the feature barcode.]' \
            '--noQuant[Don'\''t run downstream barcode-salmon model.]' \
            '--numCellBootstraps[Generate mean and variance for cell x gene matrix quantification estimates.]' \
            '--numCellGibbsSamples[Generate mean and variance for cell x gene matrix quantification by running gibbs chain estimates.]' \
            '--forceCells[Explicitly specify the number of cells.]' \
            '--expectCells[define a close upper bound on expected number of cells]' \
            '--mrna[path to a file containing mito-RNA gene, one per line]' \
            '--rrna[path to a file containing ribosomal RNA, one per line]' \
            '--keepCBFraction[fraction of CB to keep, value must be in range (0,1\], use 1 to quantify all CB.]' \
            '--read-geometry[format string describing the geometry of the read]' \
            '--bc-geometry[format string describing the geometry of the cell barcode]' \
            '--umi-geometry[format string describing the genometry of the umi]' \
            '--end[Cell-Barcodes end (5 or 3) location in the read sequence from where barcode has to be extracted. (end, umiLength, barcodeLength) should all be provided if using this option]' \
            '--umiLength[umi length Parameter for unknown protocol. (end, umiLength, barcodeLength) should all be provided if using this option]' \
            '--barcodeLength[barcode length Parameter for unknown protocol. (end, umiLength, barcodeLength) should all be provided if using this option]' \
            '--noem[do not run em]' \
            '--freqThreshold[threshold for the frequency of the barcodes]' \
            '--umiEditDistance[Maximum allowble edit distance to collapse UMIs, Expect delay in running time if != 1]' \
            '--dumpfq[Dump barcode modified fastq file for downstream analysis by using coin toss for multi-mapping.]' \
            '--dumpBfh[dump the big hash with all the barcodes and the UMI sequence.]' \
            '--dumpArborescences[dump the gene-v-cell matrix for the total number of fragments used in the UMI deduplicaiton.]' \
            '--dumpUmiGraph[dump the per cell level Umi Graph.]' \
            '--dumpCellEq[dump the per cell level deduplicated equivalence classes.]' \
            '--dumpFeatures[Dump features for whitelist and downstream analysis.]' \
            '--dumpMtx[Dump cell v transcripts count matrix in sparse mtx format.]' \
            '--lowRegionMinNumBarcodes[Minimum Number of CB to use for learning Low confidence region (Default: 200).]' \
            '--maxNumBarcodes[Maximum allowable limit to process the cell barcodes. (Default: 100000)]' \
            "*: :_files"

    }

    function _salmon_swim {
        _arguments \
            "*: :_files"

    }

    function _salmon_quantmerge {
        _arguments \
            {-v,--version}'[print version string]' \
            {-h,--help}'[produce help message]' \
            '--quants[List of quantification directories.]' \
            '--names[Optional list of names to give to the samples.]' \
            {-c,--column}'[The name of the column that will be merged together into the output files. The options are {len, elen, tpm, numreads} (default: tpm)]' \
            '--genes[Use gene quantification instead of transcript.]' \
            '--missing[The value of missing values. (default: NA)]' \
            {-o,--output}'[Output quantification file.]' \
            "*: :_files"

    }


function _salmon {
    local line state

    function _commands {
        local -a commands
        commands=(
            'index:create a salmon index'
            'quant:quantify a sample'
            'alevin:single cell analysis'
            'swim:perform super-secret operation'
            'quantmerge:merge multiple quantifications into a single file'
        )
        _describe 'command' commands
    }
 

    _arguments -C \
        ': :->cmd' \
        '*:: :->subcmd'

    case $state in
    (cmd)
        _commands
        ;;
    (subcmd)
        case $line[1] in
        (index)
            _salmon_index
            ;;

        (quant)
            _salmon_quant
            ;;

        (alevin)
            _salmon_alevin
            ;;

        (swim)
            _salmon_swim
            ;;

        (quantmerge)
            _salmon_quantmerge
            ;;

        esac
        ;;
     esac

}

